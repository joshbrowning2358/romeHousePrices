cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #r
cap
cap <- gsub("^[a-b]+","",cap)
cap
cap <- gsub("^[a-bA-B]+","",cap)
cap
cap <- gsub("^[a-b][A-B]+","",cap)
cap
cap <- gsub("^[A-Za-z]","",cap)
cap
cap <- gsub("^/[A-Za-z]","",cap)
cap
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
cap
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#       cap <- gsub("^[A-Za-z]","",cap)
#       cap <- gsub("^/[A-Za-z]","",cap)
#       #scrape description - useless really unless there's an email address
#       description <- html_nodes(htmlCode,".ag_descrizione")
#       description <- html_text(description)
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
cap
test <- as.numeric(cap)
test
cap <- as.numeric(cap)
i
#set base url for scraping
baseUrl <- "http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html?pag="
data <- data.table(agency = as.character(),
indirizzo = as.character(),
cap = as.numeric())
for(i in 1:numPages){
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
#       #scrape description - useless really unless there's an email address
#       description <- html_nodes(htmlCode,".ag_descrizione")
#       description <- html_text(description)
data.t <- data.table(agency = names,
indirizzo = indirizzi,
cap = cap)
data <- rbindlist(data,data.t, use.names =  TRUE fill = TRUE)
}
data
data.t <- data.table(agency = names,
indirizzo = indirizzi,
cap = cap)
data.t
data
rbindlist(data,data.t, use.names =  TRUE fill = TRUE)
data <- rbindlist(data,data.t, use.names =  TRUE, fill = TRUE)
data <- rbindlist(data, data.t, use.names =  TRUE, fill = TRUE)
data.t
#set base url for scraping
baseUrl <- "http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html?pag="
data <- data.table(agency = as.character(),
indirizzo = as.character(),
cap = as.numeric())
for(i in 1:numPages){
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
#       #scrape description - useless really unless there's an email address
#       description <- html_nodes(htmlCode,".ag_descrizione")
#       description <- html_text(description)
data.t <- data.table(agency = names,
indirizzo = indirizzi,
cap = cap)
data <- rbindlist(data, data.t, use.names =  TRUE, fill = TRUE)
}
data.t
data
data.t <- c(agency,indirizzi,cap)
data.t <- c(names,indirizzi,cap)
data.t
data.t <- list(names,indirizzi,cap)
data.t
baseUrl <- "http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html?pag="
data <- list()
for(i in 1:numPages){
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
#       #scrape description - useless really unless there's an email address
#       description <- html_nodes(htmlCode,".ag_descrizione")
#       description <- html_text(description)
data.t <- list(names,indirizzi,cap)
data[[i]] <- data.t
}
data
url <- paste0(baseUrl,i)
htmlCode
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
cap <- as.numeric(cap)
data.t <- list(names,indirizzi,cap)
data[[i]] <- data.t
data
do.call("rbind",data)
data<- do.call("rbind",data)
data
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
#       #scrape description - useless really unless there's an email address
#       description <- html_nodes(htmlCode,".ag_descrizione")
#       description <- html_text(description)
data.t <- list(names,indirizzi,cap)
data[[i]] <- data.t
data
data.t
data[i] <- data.t
data[[i]] <- data.t
data<- do.call("rbindlist",data)
data<- do.call("rbindlist, use.names = TRUE",data)
data<- do.call("rbind",data)
data
cap <- html_text(details)
cap
#set wd
setwd("~/Dropbox/FAO ESS STUFF/EAR")
#setwd("C:/Users/Michael/Dropbox/FAO ESS STUFF/EAR") #home
setwd("C:/Users/rahija/Dropbox/FAO ESS STUFF/EAR")
library(dplyr)
data <- read.csv("relevant_variables.csv", stringsAsFactors = FALSE)
colnames(data)
data[,25] #project titles
data[,2]  #NTE
unique(data[,2])
##clean up column names
cols <- colnames(data)
cols <- gsub("\\.","",cols)
colnames(data) <- cols
head(data)
colnames(data)
#data$ActualEOD <- as.POSIXct(data$ActualEOD,"YYYY-MM-DD")
data$ActualNTE <- as.POSIXct(data$ActualEOD,"YYYY-MM-DD")
data$ActualNTE <-as.Date(data$ActualNTE, format = "%d/%b/%Y")
data.o <- filter(data, ProjectStatus == "Operationally Active")
length(data.o)
data$ActualNTE <-as.Date(data$ActualNTE, format = "%d/%b/%Y")
data.o <- filter(data, ProjectStatus == "Operationally Active")
data.o <- unique(data.o)
data.o$DonorandFundingSource <- gsub(" *","",data.o$DonorandFundingSource)
data.t <- filter(data,ActualNTE > as.Date("2015-01-01"))
test <- data.o %>%
group_by(CountryName) %>%
summarize(Number = length(unique(ProjectTitle)))
test
donor <- data.o %>%
group_by(Donor) %>%
summarize(Number =length(unique(ProjectTitle)))
donor
donor
2500*80
url
url = "http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html?pag=1"
numPages <- getAgenzieNumImmobiliare(url)
numPages
baseUrl <- "http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html?pag="
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
data.t <- list(names,indirizzi,cap)
library(rvest)
library(dplyr)
library(data.table)
#mark time start script for saving datasets
time = gsub("(-|:| )", "\\.", Sys.time())
#set wd
if(Sys.info()[4] == "JOSH_LAPTOP"){
workingDir = "C:/Users/rockc_000/Documents/GitHub/romeHousePrices/"
} else if(Sys.info()[4] == "joshua-Ubuntu-Linux"){
workingDir = "~/Documents/Github/romeHousePrices"
} else if(Sys.info()[4] =="Michaels-MacBook-Pro-2.local"||
Sys.info()[4] == "Michaels-MBP-2.lan"){
workingDir = "~/Dropbox/romeHousePrices/"        #for michael's mac yo
} else {
stop("No directory for current user!")
}
setwd(workingDir)
files = dir(path = paste0(workingDir, "/R"), full.names = TRUE)
sapply(files, source)
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
data.t <- list(names,indirizzi,cap)
data.t
head(data)
data <- list()
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
data.t <- list(names,indirizzi,cap)
data
data.t
i
data[[i]] <- data.t
i = 2
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
data.t <- list(names,indirizzi,cap)
data[[i]] <- data.t
data[[2]]
data[[1]]
finalData = rbindlist(data, fill = TRUE)
data[[2]]
attr(data)
attributes(data)
names(Data)
names(data)
finalData = do.call("rbind",data)
finalData
i = 1
baseUrl <- "http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html?pag="
data <- list()
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
data.t <- list(names = names,indirizzi = indirizzi,cap = cap)
data[[i]] <- data.t
data
i=2
#Check to make sure URL works
url <- paste0(baseUrl,i)
fail = try({
htmlCode = html(url)
})
if(is(fail, "try-error")){
warning(paste0("Page",i,"could not be read!  Returning NULL."))
return(NULL)
}
#scrape names from url
names <- html_nodes(htmlCode, ".annuncio_title a")
names <- html_text(names)
#scrape address
details <- html_nodes(htmlCode, ".luogo ")
indirizzi <- html_text(details)
indirizzi = gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    ", "", indirizzi)
indirizzi = gsub("\t\t.*","",indirizzi)
#scrape CAP
cap <- html_text(details)
cap <- gsub("^\r\n\t\t\t\t\t\t \t\r\n\t\t\t\t    .* [0-9]+", "", cap) #remove prefix
cap <- gsub("\t+","",cap) #remove "\t"s
cap <- gsub(" - .*","",cap) #remove end part of string
#remove five characters from the right
source("R/substrRight.R")
cap <- substrRight(cap, n = 5)
#convert to numeric
cap <- as.numeric(cap)
data.t <- list(names = names,indirizzi = indirizzi,cap = cap)
data[[i]] <- data.t
finalData = do.call("rbind",data)
finalData
finalData = rbindlist(data,fill = TRUE)
finalData
setwd(workingDir)
files = dir(path = paste0(workingDir, "/R"), full.names = TRUE)
sapply(files, source)
url = "http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html?pag=1"
numPages <- getAgenzieNumImmobiliare(url)
agencies <- getAgenzieDetailsImmobiliare(numPages)
agencies
summary(agencies$cap)
sum(duplicated(agencies$names))
unique(agencies$cap)
tab <- agencies %>% group_by(cap) %>% summarize(length(names))
tab
tab
tab
as.data.frame(tab)
tab <- agencies %>%
group_by(cap) %>%
summarize(length(unique(names)))
tab
as.data.frame(tab)
sum(tab$[,2])
sum(tab[,2])
tab <- as.data.frame(tab)
head(tab)
sum(tab[,2])
